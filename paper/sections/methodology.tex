% Methodology
% Fully satisfies assignment requirements

\label{sec:methodology}

\subsection{Overview and Research Design}
Our motivating research question asks how Wikipedia maintains engagement and public interest without the help of likes, follower counts, or algorithmic recommendations. Since these ideas cannot be measured directly, we created a more specific operationalized research question that we can answer with data:

\textbf{Operationalized Research Question:} 
\emph{How much sustained public attention do selected Wikipedia articles receive over a 30-day period, based on their average daily pageviews?}

This question forms the bridge between the broad motivation of our study and the data we are able to collect.

\subsection{Proxy Selection and Justification}
Because concepts like participation, visibility, and motivation are not directly observable, we use pageviews as a measurable proxy for public attention. Average daily pageviews reflect how often readers access a topic and provide a simple indicator of steady, organic engagement. Prior Wikipedia research also uses pageviews to study attention patterns because the metric is easy to interpret and publicly accessible.

\subsection{Data Collection}
We collected data using the Wikimedia Pageviews REST API. Our Python script (\texttt{week7.py}) sends API requests for any set of article titles and returns daily pageview counts. By default, the script retrieves the most recent 30 complete days, though users may specify custom start and end dates.

For each article, the API provides:
\begin{itemize}
    \item Daily pageviews,
    \item The total number of views during the collection window, and
    \item The number of valid days returned by the API.
\end{itemize}

The script prints a formatted comparison table and automatically saves all results to \texttt{results\_week7.csv}. Our full repository, including all code, is available at:  
\url{https://github.com/tmomani11/CPSC298-12}.

\subsection{Data Processing}
The scriptâ€™s \texttt{summarize()} function computes the basic statistics used in our analysis. In addition to the total number of views and days of data, it calculates the \textbf{average daily pageviews}, which we use as the main comparison metric. All data are stored in a consistent format to support reproducibility.

\subsection{Reproducibility}
All code and data files are provided in the project repository. Anyone can reproduce our results using the following command:

\begin{verbatim}
python week7.py "Artificial intelligence" "Climate change"
\end{verbatim}

Running this command regenerates the comparison table and produces a new \texttt{results\_week7.csv} file.
