---
type: research-workflow
version: 1.1
author: Dylan Massaro
course: CPSC 298 — Wikipedia Research 2025
created: 2025-10-27
description: >-
  This workflow explains how I plan to use an AI assistant to help with the
  literature review for my project on Wikipedia’s collaboration and governance.
  It focuses on collecting and summarizing research about how Wikipedia works as
  a social platform and how it compares to other sites like Reddit or Twitter.
prerequisites:
  - Research questions are finalized (see README.md)
  - At least 5 sources saved in literature.md
  - Working access to the Wikipedia API (week7.py)
  - Internet access for Google Scholar or Semantic Scholar searches
  - `references.bib` ready to store new citations
---

# Overview
This workflow will guide an AI tool to help find and summarize academic sources
that relate to Wikipedia’s collaboration and governance systems. The main goal
is to better understand how Wikipedia functions as a kind of social media
platform, how it differs from other platforms, and how its structure encourages
productive collaboration. The AI assistant will help identify relevant papers,
extract key points, and organize results into consistent summaries for my
literature review.

# Research Questions
1. How does the absence of likes, follower counts, and similar features affect participation and motivation on Wikipedia?
2. How do talk pages, edit wars, and bursty editing patterns show both conflict and collaboration?
3. Which parts of Wikipedia’s design (like transparency, governance, and norms) help people work together effectively?
4. How could these same design choices help improve collaboration or trust on other platforms such as Reddit or Twitter?

# Relevance Criteria
A paper is relevant if it meets one or more of these:
- Discusses Wikipedia’s collaboration, governance, or community norms
- Examines editor motivation or participation dynamics
- Compares Wikipedia with other online communities (e.g., Reddit, Twitter)
- Uses data or methods that can be applied to studying governance or trust
- Focuses on information quality, moderation, or self-regulation online

# Input
The AI will receive:
- Example papers listed in `literature.md`
- Keywords such as “Wikipedia governance,” “Wikipedia vs Reddit,” “online collaboration,” and “community moderation”
- Optional time range: 2005–2025

# Output
For each paper, the AI should include:
- Full citation (APA style)
- DOI or URL
- Study type (empirical, theoretical, review, etc.)
- Platform (Wikipedia, Reddit, Twitter, or multiple)
- Main methods (for example: network analysis, regression, temporal modeling)
- Main findings (3–5 short bullet points)
- Limitations (1–2 short points)
- Which research questions the paper relates to
- Quality note (e.g., peer-reviewed, open data)
- Relevance score from 1–5

# Sources to Check
- Google Scholar, Semantic Scholar, or OpenAlex
- Wikipedia/Wikimedia data (policies, talk pages, ORES, API docs)
- CSCW, CHI, ICWSM, WebSci conference papers
- Journals like JASIST, PNAS, or Nature Human Behaviour
- Papers comparing governance or moderation systems across platforms

# Instructions for the AI
1. Search for about 15–25 possible sources using the keywords.
2. Filter by the relevance criteria above.
3. Summarize each paper clearly and avoid speculation.
4. Connect findings to my research questions where possible.
5. Create both Markdown summaries (for the literature folder) and JSON output (for data storage).

# Constraints
- Each summary should be around 200 words or less.
- Only include reliable and verifiable academic sources.
- Keep consistent phrasing for methods and research questions.
- Mark missing info as “unknown.”

# Output Format
### Markdown (for `literature/literature-review.md`)
```
### Title (Year) — Venue
**Authors:** …  
**Platform:** Wikipedia | Reddit | Twitter | Multi  
**Methods:** …  
**Findings:**
- …
- …
**Limitations:** …  
**Relevance:** RQ1 (…), RQ2 (…), RQ3 (…), RQ4 (…)  
**Links:** DOI/URL  
**Score:** 1–5
```

### JSON (for `data/lit/records.json`)
```json
{
  "title": "…",
  "year": 2024,
  "platform": "Wikipedia",
  "methods": ["network analysis"],
  "findings": ["…"],
  "limitations": ["…"],
  "relevance_to_RQ": {"RQ1": "…", "RQ3": "…"},
  "score": 5,
  "url": "https://…"
}
```

# Verification
- Check that all DOIs or URLs work.
- Make sure findings match what the paper actually says.
- Include a balance of Wikipedia-only and comparative studies.
- Verify summaries match the project’s main research focus.

# Example Summary
### “Edit Wars in Wikipedia” (Sumi et al., 2011 — arXiv)
**Platform:** Wikipedia  
**Methods:** Conflict detection, edit/revert analysis  
**Findings:**
- Conflicts cluster around topics like politics and religion.
- Revert bursts show when disagreements escalate.
- Talk pages help resolve disputes.  
**Limitations:** Older data, doesn’t compare with other platforms.  
**Relevance:** RQ2 (conflict), RQ3 (governance).  
**Score:** 5  
**Links:** [pdf](https://arxiv.org/pdf/1107.3689)

# Session Notes
- Limit each run to 10–15 papers to avoid duplicates.
- Append new summaries instead of overwriting.
- Double-check new sources before adding them to the final bibliography.

